{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "Create an LDA topic model from scratch using collapsed Gibbs Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/rca2t/COURSES/DSI/DS5559/UVA_DSI_REPO'\n",
    "local_lib = base_path + '/lib'\n",
    "src_dir = base_path + '/play/lda/corpora'\n",
    "corpus_db = \"20news.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs = 50\n",
    "n_topics = 20\n",
    "n_iters = 100\n",
    "alpha = 1\n",
    "beta = .001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import random\n",
    "import sys; sys.path.append(local_lib)\n",
    "import textman.textman as tx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT * FROM doc ORDER BY RANDOM() LIMIT ?\"\n",
    "with sqlite3.connect(src_dir + \"/\" + corpus_db) as db:\n",
    "    docs = pd.read_sql(sql, db, index_col='doc_id', params=(n_docs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set([topic for group in docs.doc_label.unique() for topic in group.split('.')[1:] if topic not in ['x']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'politics windows mac ms-windows space os med autos hardware mideast misc sport baseball ibm christian pc crypt sys electronics motorcycles religion hockey'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comp.sys.ibm.pc.hardware    8\n",
       "sci.electronics             5\n",
       "rec.sport.hockey            5\n",
       "comp.sys.mac.hardware       5\n",
       "soc.religion.christian      4\n",
       "rec.motorcycles             4\n",
       "rec.autos                   4\n",
       "talk.politics.misc          3\n",
       "comp.windows.x              2\n",
       "talk.religion.misc          2\n",
       "sci.crypt                   2\n",
       "sci.med                     2\n",
       "rec.sport.baseball          1\n",
       "comp.os.ms-windows.misc     1\n",
       "sci.space                   1\n",
       "talk.politics.mideast       1\n",
       "Name: doc_label, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.doc_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_topics = len(labels)\n",
    "# n_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert corpus to tokens and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab = tx.create_tokens_and_vocab(docs, src_col='doc_content')\n",
    "tokens['token_num'] = tokens.groupby(['doc_id']).cumcount()\n",
    "tokens = tokens.reset_index()[['doc_id','token_num','term_id']]\n",
    "tokens = tokens[tokens.term_id.isin(vocab[vocab.go].index)]\n",
    "tokens = tokens.set_index(['doc_id','token_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_str'] = tokens.term_id.map(vocab.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = \"\"\"people one would know like men think two see well said years guy things want let get back new first make even take \n",
    "going right way could everything need says also look got made say used man every real anotherthree use help means good \n",
    "never took went tell day old ever told nobody show knew nothing five big fit still turn give found later began \n",
    "another sure since mit edu com gov however\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[~tokens.term_str.isin(sw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>term_id</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">52801</th>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2924</td>\n",
       "      <td>writes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1719</td>\n",
       "      <td>might</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>258</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2940</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  term_id term_str\n",
       "doc_id token_num                  \n",
       "52801  0              386  article\n",
       "       3             2924   writes\n",
       "       4             1719    might\n",
       "       5              258      add\n",
       "       6             2940     year"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce tokens to unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokens.groupby(['doc_id','term_id']).count().rename(columns={'term_str':'n'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.DataFrame(index=range(n_topics))\n",
    "topics.index.name = 'topic_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly assign topics to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_topics(tokens):\n",
    "    tokens['topic_id'] = np.random.randint(0, n_topics, len(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = init_topics(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>term_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">52801</th>\n",
       "      <th>0</th>\n",
       "      <td>386</td>\n",
       "      <td>article</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2924</td>\n",
       "      <td>writes</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1719</td>\n",
       "      <td>might</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>258</td>\n",
       "      <td>add</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2940</td>\n",
       "      <td>year</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  term_id term_str  topic_id\n",
       "doc_id token_num                            \n",
       "52801  0              386  article         4\n",
       "       3             2924   writes        19\n",
       "       4             1719    might        16\n",
       "       5              258      add        10\n",
       "       6             2940     year        13"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Count Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DT(tokens):\n",
    "    return tokens.groupby(['doc_id','topic_id']).topic_id.count()\\\n",
    "        .unstack().fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WT(tokens):\n",
    "    return tokens.groupby(['topic_id','term_id']).term_id.count()\\\n",
    "        .unstack().fillna(0).astype('int').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(dt):\n",
    "    m = np.ones(topics.shape[0])\n",
    "    theta = dt.apply(lambda x: pd.Series(np.random.dirichlet(alpha * m)), 1)\n",
    "    theta.columns.name = 'topic_id'\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 1\n",
    "\n",
    "See [this blog](http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define row function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_topic(row):\n",
    "    \n",
    "    d = row.name[0] # doc_id\n",
    "    k = row.name[1] # token_num\n",
    "    z = row.topic_id\n",
    "    w = row.term_id\n",
    "    \n",
    "    DT.at[d,z] = DT.at[d,z] - 1\n",
    "    WT.at[w,z] = WT.at[w,z] - 1\n",
    "    \n",
    "    DTP = DT.loc[d] / DT.loc[d].sum()\n",
    "    WTP = WT.loc[w] / WT.sum() #WT.loc[w].sum()\n",
    "        \n",
    "    p_z = DTP * WTP\n",
    "    z_weights = p_z / p_z.sum()\n",
    "\n",
    "    z1 = topics.sample(weights=z_weights).index.values[0]\n",
    "\n",
    "    DT.at[d,z1] = DT.at[d,z1] + 1\n",
    "    WT.at[w,z1] = WT.at[w,z1] + 1\n",
    "    \n",
    "    return z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = init_topics(tokens)\n",
    "DT = get_DT(tokens) + alpha\n",
    "WT = get_WT(tokens) + beta\n",
    "for i in range(n_iters):\n",
    "    print(i, end=' ')\n",
    "    tokens['topic_id'] = tokens.apply(get_new_topic, 1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0: drive mercury based nasa disks file possible likely according colorado\n",
      "TOPIC 1: thing israel internet using remember lebanon robert try bosnian original\n",
      "TOPIC 2: might soc system christopher branch ultra read put msg heavy\n",
      "TOPIC 3: etc true body long save probably allergic happy buy perfect\n",
      "TOPIC 4: better case please support yeast boston executive general responsible course\n",
      "TOPIC 5: luke opinions least subject email government west suggest net sci\n",
      "TOPIC 6: culture non graphics due compound knowledge far life windows computer\n",
      "TOPIC 7: article writes world corn find left military posted gordon tanks\n",
      "TOPIC 8: information already biden ago chips maybe embargo listserv area start\n",
      "TOPIC 9: post around fine article bonds little willing request fast orbit\n",
      "TOPIC 10: technology bad point anything agents getting word place apparently security\n",
      "TOPIC 11: card may isa bit seems mode evidence account sodium matthew\n",
      "TOPIC 12: writes article last christian almost david astronomy deal simple build\n",
      "TOPIC 13: cards set able called color vga video memory war bios\n",
      "TOPIC 14: uga problems else info matter pretty radio whether georgia come\n",
      "TOPIC 15: many much work ati lot related yes mail available quite\n",
      "TOPIC 16: john problem year law part generally women opinion mass posting\n",
      "TOPIC 17: mark time uucp bus best version ram atf oilers response\n",
      "TOPIC 18: space writes different logic eisa acts lucky serbian claim humanitarian\n",
      "TOPIC 19: anyone must knows electronics telescope heart software person god idea\n"
     ]
    }
   ],
   "source": [
    "print_topics(WT, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_phi(wt):\n",
    "    n = np.ones(wt.shape[0])\n",
    "    phi = wt.apply(lambda x: pd.Series(np.random.dirichlet(beta * n)))\n",
    "    phi.index.name = 'term_id'\n",
    "    phi.index = wt.index\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(wt, n=5):    \n",
    "    wtx = (WT / WT.sum())\n",
    "    wtx['term_str'] = vocab.term\n",
    "    wtx = wtx.set_index('term_str')\n",
    "    for t in topics.index:\n",
    "        T = wtx[t]\n",
    "#         T = T[T > alpha]\n",
    "        print('TOPIC', t,  end=': ')\n",
    "        try:\n",
    "            print(' '.join(T.sort_values(ascending=False).head(n).index.values))\n",
    "        except:\n",
    "            print(\"NO DICE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_2(row):\n",
    "    \n",
    "    d = row.name[0] # doc_id\n",
    "    k = row.name[1] # token_num\n",
    "    z = row.topic_id\n",
    "    w = row.term_id\n",
    "    \n",
    "    p_z = np.exp(np.log(THETA.loc[d]) + np.log(PHI.loc[w]))\n",
    "    p_z = THETA.loc[d] * PHI.loc[w]\n",
    "    p_z = p_z / np.sum(p_z)\n",
    "    z1 = np.random.multinomial(1, p_z).argmax()\n",
    "    \n",
    "    return z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = init_topics(tokens)\n",
    "DT = get_DT(tokens)\n",
    "WT = get_WT(tokens)\n",
    "THETA = DT.apply(lambda x: pd.Series(np.random.dirichlet(alpha + x)), 1)\n",
    "PHI = WT.apply(lambda x: pd.Series(np.random.dirichlet(beta + x)))\n",
    "PHI.index = WT.index\n",
    "\n",
    "for i in range(n_iters):\n",
    "    print(i, end=' ')\n",
    "    tokens['topic_id'] = tokens.apply(method_2, 1)\n",
    "    DT = get_DT(tokens)\n",
    "    WT = get_WT(tokens)\n",
    "    THETA = DT.apply(lambda x: pd.Series(np.random.dirichlet(alpha + x)), 1)\n",
    "    PHI = WT.apply(lambda x: pd.Series(np.random.dirichlet(beta + x)))\n",
    "    PHI.index = WT.index\n",
    "\n",
    "print()\n",
    "print_topics(WT, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## METHOD 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute `P(topic|document)`\n",
    "\n",
    "`p(topic t | document d)` Where proportion of words in document d that are assigned to topic t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_tGd(tokens):\n",
    "    p_tGd = tokens.reset_index().groupby(['doc_id','topic_id']).token_num.count().to_frame().unstack().fillna(0)\n",
    "    p_tGd = p_tGd.apply(lambda x: x / x.sum(), axis=1)\n",
    "    p_tGd.columns = p_tGd.columns.droplevel(0)\n",
    "    return p_tGd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tGd = get_p_tGd(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tGd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute `P(word|topic)`\n",
    "`p(word w | topic t)` Where proportion of assignments to topic t, over all documents d, that come from word w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_wGt(tokens):\n",
    "    p_wGt = tokens.reset_index().groupby(['topic_id','term_id']).token_num.count().to_frame().unstack().fillna(0)\n",
    "    p_wGt = p_wGt.apply(lambda x: x / x.sum(), axis=1)\n",
    "    p_wGt.columns = p_wGt.columns.droplevel(0)\n",
    "    return p_wGt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wGt = get_p_wGt(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wGt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best topic \n",
    "\n",
    "`p(topic t’ | document d) * p(word w | topic t’)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_topic(row, ptd, pwt):\n",
    "    doc_id = row.name[0]\n",
    "    token_num = row.name[1]\n",
    "    term_id = row.term_id\n",
    "    results = [ptd.at[topic_id, doc_id] * pwt.at[term_id, topic_id] for topic_id in topics.index]\n",
    "    new_topic = results.index(max(results))\n",
    "    return new_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = init_topics(tokens)\n",
    "for i in range(n_iters):\n",
    "    print(i, end=' ')\n",
    "    DT = get_DT(tokens)\n",
    "    WT = get_WT(tokens)\n",
    "    PTD = (DT.T / DT.T.sum())\n",
    "    PWT = (WT / WT.sum())\n",
    "    tokens['topic_id'] = tokens.apply(lambda row: get_best_topic(row, PTD, PWT), axis=1)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = tokens.groupby(['topic_id','term_id']).term_id.count().to_frame().unstack().fillna(0).T\n",
    "TOPICS.index = TOPICS.index.droplevel(0)\n",
    "TOPICS['term_str'] = vocab.term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_id in topics.index:\n",
    "    print(topic_id, ', '.join(TOPICS.sort_values(topic_id, ascending=False).head(10).term_str.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "789px",
    "left": "0px",
    "right": "1186px",
    "top": "111px",
    "width": "254px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
